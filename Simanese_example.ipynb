{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simanese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudarshan-s-harithas/SiameseCNN/blob/master/Simanese_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-LmaYIZ8a2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "import errno\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets.mnist\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmPiA-MC8gaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do_learn = True\n",
        "save_frequency = 2\n",
        "batch_size = 16\n",
        "lr = 0.001\n",
        "num_epochs = 10\n",
        "weight_decay = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cra97EmM8hDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_int(b):\n",
        "   return int(codecs.encode(b, 'hex'), 16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS8btPIm8jSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_label_file(path):\n",
        "   with open(path, 'rb') as f:\n",
        "      data = f.read()\n",
        "   assert get_int(data[:4]) == 2049\n",
        "   length = get_int(data[4:8])\n",
        "   parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
        "   return torch.from_numpy(parsed).view(length).long()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpA-3EW48ky1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_image_file(path):\n",
        "   with open(path, 'rb') as f:\n",
        "      data = f.read()\n",
        "   assert get_int(data[:4]) == 2051\n",
        "   length = get_int(data[4:8])\n",
        "   num_rows = get_int(data[8:12])\n",
        "   num_cols = get_int(data[12:16])\n",
        "   images = []\n",
        "   parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
        "   return torch.from_numpy(parsed).view(length, num_rows, num_cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XDsir9k8mle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BalancedMNISTPair(torch.utils.data.Dataset):\n",
        "   \"\"\"Dataset that on each iteration provides two random pairs of\n",
        "   MNIST images. One pair is of the same number (positive sample), one\n",
        "   is of two different numbers (negative sample).\n",
        "   \"\"\"\n",
        "   urls = [\n",
        "      'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
        "      'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
        "      'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
        "      'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
        "   ]\n",
        "   raw_folder = 'raw'\n",
        "   processed_folder = 'processed'\n",
        "   training_file = 'training.pt'\n",
        "   test_file = 'test.pt'\n",
        "   \n",
        "   def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "      self.root = os.path.expanduser(root)\n",
        "      self.transform = transform\n",
        "      self.target_transform = target_transform\n",
        "      self.train = train # training set or test set\n",
        "      \n",
        "      if download:\n",
        "         self.download()\n",
        "         \n",
        "      if not self._check_exists():\n",
        "         raise RuntimeError('Dataset not found.' + ' You can use download=True to download it')\n",
        "         \n",
        "      if self.train:\n",
        "         self.train_data, self.train_labels = torch.load(\n",
        "            os.path.join(self.root, self.processed_folder, self.training_file))\n",
        "         \n",
        "         train_labels_class = []\n",
        "         train_data_class = []\n",
        "         for i in range(10):\n",
        "            indices = torch.squeeze((self.train_labels == i).nonzero())\n",
        "            train_labels_class.append(torch.index_select(self.train_labels, 0, indices))\n",
        "            train_data_class.append(torch.index_select(self.train_data, 0, indices))\n",
        "            \n",
        "         # generate balanced pairs\n",
        "         self.train_data = []\n",
        "         self.train_labels = []\n",
        "         lengths = [x.shape[0] for x in train_labels_class]\n",
        "         for i in range(10):\n",
        "            for j in range(500): # create 500 pairs\n",
        "               rnd_cls = random.randint(0,8) # choose random class that is not the same class\n",
        "               if rnd_cls >= i:\n",
        "                  rnd_cls = rnd_cls + 1\n",
        "\n",
        "               rnd_dist = random.randint(0, 100)\n",
        "                  \n",
        "               self.train_data.append(torch.stack([train_data_class[i][j], train_data_class[i][j+rnd_dist], train_data_class[rnd_cls][j]]))\n",
        "               self.train_labels.append([1,0])\n",
        "\n",
        "         self.train_data = torch.stack(self.train_data)\n",
        "         self.train_labels = torch.tensor(self.train_labels)\n",
        "               \n",
        "      else:\n",
        "         self.test_data, self.test_labels = torch.load(\n",
        "            os.path.join(self.root, self.processed_folder, self.test_file))\n",
        "         \n",
        "         test_labels_class = []\n",
        "         test_data_class = []\n",
        "         for i in range(10):\n",
        "            indices = torch.squeeze((self.test_labels == i).nonzero())\n",
        "            test_labels_class.append(torch.index_select(self.test_labels, 0, indices))\n",
        "            test_data_class.append(torch.index_select(self.test_data, 0, indices))\n",
        "            \n",
        "         # generate balanced pairs\n",
        "         self.test_data = []\n",
        "         self.test_labels = []\n",
        "         lengths = [x.shape[0] for x in test_labels_class]\n",
        "         for i in range(10):\n",
        "            for j in range(500): # create 500 pairs\n",
        "               rnd_cls = random.randint(0,8) # choose random class that is not the same class\n",
        "               if rnd_cls >= i:\n",
        "                  rnd_cls = rnd_cls + 1\n",
        "\n",
        "               rnd_dist = random.randint(0, 100)\n",
        "                  \n",
        "               self.test_data.append(torch.stack([test_data_class[i][j], test_data_class[i][j+rnd_dist], test_data_class[rnd_cls][j]]))\n",
        "               self.test_labels.append([1,0])\n",
        "\n",
        "         self.test_data = torch.stack(self.test_data)\n",
        "         self.test_labels = torch.tensor(self.test_labels)\n",
        "         \n",
        "   def __getitem__(self, index):\n",
        "      if self.train:\n",
        "         imgs, target = self.train_data[index], self.train_labels[index]\n",
        "      else:\n",
        "         imgs, target = self.test_data[index], self.test_labels[index]\n",
        "         \n",
        "      img_ar = []\n",
        "      for i in range(len(imgs)):\n",
        "         img = Image.fromarray(imgs[i].numpy(), mode='L')\n",
        "         if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "         img_ar.append(img)\n",
        "         \n",
        "      if self.target_transform is not None:\n",
        "         target = self.target_transform(target)\n",
        "         \n",
        "      return img_ar, target\n",
        "   \n",
        "   def __len__(self):\n",
        "      if self.train:\n",
        "         return len(self.train_data)\n",
        "      else:\n",
        "         return len(self.test_data)\n",
        "      \n",
        "   def _check_exists(self):\n",
        "      return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
        "         os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
        "   \n",
        "   def download(self):\n",
        "      \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
        "      from six.moves import urllib\n",
        "      import gzip\n",
        "\n",
        "      if self._check_exists():\n",
        "         return\n",
        "\n",
        "      # download files\n",
        "      try:\n",
        "         os.makedirs(os.path.join(self.root, self.raw_folder))\n",
        "         os.makedirs(os.path.join(self.root, self.processed_folder))\n",
        "      except OSError as e:\n",
        "         if e.errno == errno.EEXIST:\n",
        "            pass\n",
        "         else:\n",
        "            raise\n",
        "\n",
        "      for url in self.urls:\n",
        "         print('Downloading ' + url)\n",
        "         data = urllib.request.urlopen(url)\n",
        "         filename = url.rpartition('/')[2]\n",
        "         file_path = os.path.join(self.root, self.raw_folder, filename)\n",
        "         with open(file_path, 'wb') as f:\n",
        "            f.write(data.read())\n",
        "         with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
        "               gzip.GzipFile(file_path) as zip_f:\n",
        "            out_f.write(zip_f.read())\n",
        "         os.unlink(file_path)\n",
        "\n",
        "      # process and save as torch files\n",
        "      print('Processing...')\n",
        "\n",
        "      training_set = (\n",
        "         read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
        "         read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
        "      )\n",
        "      test_set = (\n",
        "         read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
        "         read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
        "      )\n",
        "      with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
        "         torch.save(training_set, f)\n",
        "      with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
        "         torch.save(test_set, f)\n",
        "\n",
        "      print('Done!')\n",
        "\n",
        "   def __repr__(self):\n",
        "      fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "      fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "      tmp = 'train' if self.train is True else 'test'\n",
        "      fmt_str += '    Split: {}\\n'.format(tmp)\n",
        "      fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "      tmp = '    Transforms (if any): '\n",
        "      fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "      tmp = '    Target Transforms (if any): '\n",
        "      fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "      return fmt_str\n",
        "      \n",
        "class Net(nn.Module):\n",
        "   def __init__(self):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.conv1 = nn.Conv2d(1, 64, 7)\n",
        "      self.pool1 = nn.MaxPool2d(2)\n",
        "      self.conv2 = nn.Conv2d(64, 128, 5)\n",
        "      self.conv3 = nn.Conv2d(128, 256, 5)\n",
        "      self.linear1 = nn.Linear(2304, 512)\n",
        "      \n",
        "      self.linear2 = nn.Linear(512, 2)\n",
        "      \n",
        "   def forward(self, data):\n",
        "      res = []\n",
        "      for i in range(2): # Siamese nets; sharing weights\n",
        "         x = data[i]\n",
        "         x = self.conv1(x)\n",
        "         x = F.relu(x)\n",
        "         x = self.pool1(x)\n",
        "         x = self.conv2(x)\n",
        "         x = F.relu(x)\n",
        "         x = self.conv3(x)\n",
        "         x = F.relu(x)\n",
        "         \n",
        "         x = x.view(x.shape[0], -1)\n",
        "         x = self.linear1(x)\n",
        "         res.append(F.relu(x))\n",
        "         \n",
        "      res = torch.abs(res[1] - res[0])\n",
        "      res = self.linear2(res)\n",
        "      return res\n",
        "   \n",
        "def train(model, device, train_loader, epoch, optimizer):\n",
        "   model.train()\n",
        "   \n",
        "   for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      for i in range(len(data)):\n",
        "         data[i] = data[i].to(device)\n",
        "         \n",
        "      optimizer.zero_grad()\n",
        "      output_positive = model(data[:2])\n",
        "      output_negative = model(data[0:3:2])\n",
        "      \n",
        "      target = target.type(torch.LongTensor).to(device)\n",
        "      target_positive = torch.squeeze(target[:,0])\n",
        "      target_negative = torch.squeeze(target[:,1])\n",
        "      \n",
        "      loss_positive = F.cross_entropy(output_positive, target_positive)\n",
        "      loss_negative = F.cross_entropy(output_negative, target_negative)\n",
        "      \n",
        "      loss = loss_positive + loss_negative\n",
        "      loss.backward()\n",
        "      \n",
        "      optimizer.step()\n",
        "      if batch_idx % 10 == 0:\n",
        "         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx*batch_size, len(train_loader.dataset), 100. * batch_idx*batch_size / len(train_loader.dataset),\n",
        "            loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "   model.eval()\n",
        "   \n",
        "   with torch.no_grad():\n",
        "      accurate_labels = 0\n",
        "      all_labels = 0\n",
        "      loss = 0\n",
        "      for batch_idx, (data, target) in enumerate(test_loader):\n",
        "         for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "            \n",
        "         output_positive = model(data[:2])\n",
        "         output_negative = model(data[0:3:2])\n",
        "            \n",
        "         target = target.type(torch.LongTensor).to(device)\n",
        "         target_positive = torch.squeeze(target[:,0])\n",
        "         target_negative = torch.squeeze(target[:,1])\n",
        "            \n",
        "         loss_positive = F.cross_entropy(output_positive, target_positive)\n",
        "         loss_negative = F.cross_entropy(output_negative, target_negative)\n",
        "            \n",
        "         loss = loss + loss_positive + loss_negative\n",
        "            \n",
        "         accurate_labels_positive = torch.sum(torch.argmax(output_positive, dim=1) == target_positive).cpu()\n",
        "         accurate_labels_negative = torch.sum(torch.argmax(output_negative, dim=1) == target_negative).cpu()\n",
        "            \n",
        "         accurate_labels = accurate_labels + accurate_labels_positive + accurate_labels_negative\n",
        "         all_labels = all_labels + len(target_positive) + len(target_negative)\n",
        "      \n",
        "      accuracy = 100. * accurate_labels / all_labels\n",
        "      print('Test accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))\n",
        "   \n",
        "def oneshot(model, device, data):\n",
        "   model.eval()\n",
        "\n",
        "   with torch.no_grad():\n",
        "      for i in range(len(data)):\n",
        "            data[i] = data[i].to(device)\n",
        "      \n",
        "      output = model(data)\n",
        "      return torch.squeeze(torch.argmax(output, dim=1)).cpu().item()\n",
        "\n",
        "def main():\n",
        "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "   trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "   \n",
        "   model = Net().to(device)\n",
        "   \n",
        "   if do_learn: # training mode\n",
        "      train_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=True, download=True, transform=trans), batch_size=batch_size, shuffle=True)\n",
        "      test_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=False, download=True, transform=trans), batch_size=batch_size, shuffle=False)\n",
        "      \n",
        "      optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "      for epoch in range(num_epochs):\n",
        "         train(model, device, train_loader, epoch, optimizer)\n",
        "         test(model, device, test_loader)\n",
        "         if epoch & save_frequency == 0:\n",
        "            torch.save(model, 'siamese_{:03}.pt'.format(epoch))\n",
        "   else: # prediction\n",
        "      prediction_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=False, download=True, transform=trans), batch_size=1, shuffle=True)\n",
        "      model.load_state_dict(torch.load(load_model_path))\n",
        "      data = []\n",
        "      data.extend(next(iter(prediction_loader))[0][:3:2])\n",
        "      same = oneshot(model, device, data)\n",
        "      if same > 0:\n",
        "         print('These two images are of the same number')\n",
        "      else:\n",
        "         print('These two images are not of the same number')\n",
        "         \n",
        "if __name__ == '__main__':\n",
        "   main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}